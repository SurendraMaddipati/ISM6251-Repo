# ISM6251-Repo
Data Science Programming

Each of the directories above consists of separate assignments and in-class exercises that cover a range of machine learning algorithms.

 **Week-1** I have acquired my skills in data cleaning and preprocessing, ensuring that the data is accurate and ready for modeling. As part of this process, I have implemented one-hot encoding and dummy encoding techniques on an Airbnb dataset, which has further enhanced my understanding of these methodologies.
 
 **Week 2**- I have gained exposure to various regression techniques during my coursework. This includes essential concepts such as simple linear regression, multivariable linear regression, polynomial regression, and logistic regression, which I have applied to the Airbnb dataset to evaluate their performance. To further experiment with these techniques, I also created a custom dataset and introduced noise to observe their effectiveness. Additionally, I have been introduced to the topic of regularization techniques such as Ridge regression, Lasso regression, early stopping, elastic net, and l2_l1. These experiences have significantly contributed to my knowledge and understanding of regression and its practical applications. I am excited to utilize this knowledge in my future projects and contribute to the field of machine learning.
 
 **Week 3**- During the third week of the course, I gained a comprehensive understanding of Support Vector Machines (SVM) and its primary kernel types, namely linear, RBF, and polynomial. I also explored the differences between SVM and logistic regression. To gain practical experience, I applied SVM to diverse datasets such as the riding mowers and cancer treatment datasets, and evaluated the performance of each model. Furthermore, I created a web interface for my SVM model, which allowed me to receive input and generate results on the likelihood of ownership. Overall, this week was instrumental in expanding my knowledge and expertise in SVM, equipping me with valuable skills in machine learning.

**Week 4**- During this week, I delved into the intricacies of decision trees and gained a comprehensive understanding of its key concepts such as bias, variance, entropy, and gini index. I also learned about hyperparameter tuning techniques like grid search cv and random search cv, which help optimize the performance of decision trees. To gain practical experience, I applied decision trees to the universal bank dataset and evaluated the performance of my model. Overall, this week was instrumental in enhancing my knowledge and proficiency in decision tree algorithms, equipping me with valuable skills in machine learning.
 
**Week 5** was dedicated to the study of Ensemble Techniques, which include popular algorithms like Bagging and Boosting. I learned about the key concepts associated with ensembling, such as Random Forest, AdaBoost, and XGBoost. To gain practical experience, I applied these techniques to a dataset and evaluated the performance of each model. Throughout the week, I discovered the importance of hyperparameter tuning in optimizing Ensemble models, and how techniques like Grid Search CV and Random Search CV can be used to fine-tune the models. Overall, Week 5 provided a deep understanding of ensembling techniques, equipping me with valuable skills to build highly accurate predictive models.

**Week-6** During the study of Text Mining, I had the opportunity to learn about various techniques such as Lemmatization, Tokenization, and Inverse Document Frequency (IDF). In addition, I gained knowledge about Part of Speech (POS) tagging, which is a technique used to identify the grammatical structure of sentences.
 To gain practical experience, I applied the Single Value Decomposition (SVD) technique and a regularization technique called Early Stopping to a NEWS dataset and evaluated the model performance. Through this exercise, I learned about the importance of feature engineering and dimensionality reduction in Text Mining. Overall, Text Mining is an interesting and essential field that equips data scientists with powerful tools to analyze and extract valuable insights from large volumes of textual data.
 
 **Week-7** During my study of neural networks, I became familiar with the basic concepts such as perceptrons and simple artificial neural networks (ANNs). I then applied this knowledge to build a more complex neural network using the MLP classifier.To evaluate the model's performance, I used a handwritten digits dataset, and I gained valuable experience in the process of selecting appropriate hyperparameters and regularization techniques.Through this work, I gained a deeper understanding of the power and versatility of neural networks in solving complex problems, as well as the importance of careful model selection and tuning.
 
 **Week-8** During my studies, I gained a strong understanding of deep neural networks and their implementation using popular frameworks like Keras and TensorFlow. To apply these concepts, I worked on a project involving image recognition using a handwritten digits dataset. Through this project, I gained hands-on experience in designing, training, and fine-tuning deep neural networks. I also evaluated the performance of my model and explored various techniques to improve its accuracy, including data augmentation and transfer learning. Overall, this experience allowed me to deepen my knowledge of deep learning and its practical applications.
  
 **Week -9** In this week, I delved into the concept of Convolutional Neural Networks (CNN). I used the Keras library and TensorFlow framework to apply CNN to an image dataset containing various types of apples. After training the model, I evaluated its performance and determined its effectiveness in identifying and classifying different types of apples in the images.

**Week -10** I gained an understanding of Recurrent Neural Networks (RNNs) and their applications in time series and sequential analysis. To apply this knowledge, I used the NCR stock data for 100 days and implemented RNN models such as LSTM and Conv1d. I stored the sequences for 9 days and predicted the 10th day's stock price, evaluating the performance of different RNN models.

**Week -11** During the Autoencoders module, I learned about the fundamental concepts of autoencoders and their ability to reconstruct input data. I applied this knowledge to the MNIST dataset and also created new images of the digit 5, with the first letter of my name. I trained the model and successfully reconstructed the MNIST digits and my new images, demonstrating the effectiveness of autoencoders in generating new data.

**Week 5 Assignment 1 and Week 8 Assignment 2**

**The business problem of the dataset is to classify whether a woman needs to undergo a caesarean section delivery or not.Based on various factors such as age, delivery no(Delivey No), delivery time(Delivery No), blood pressure(Blood of Pressure), Heart problem.This problem is important in the medical field  as it can help doctors and medical professionals make informed decisions about the delivery method for pregnant women. It can also help in reducing the number of unnecessary caesarean operations, which can have potential risks for both the mother and the child.The dataset contains 80 instances with 6 input variables (delivery no(Delivey No), delivery time(Delivery No), blood pressure(Blood of Pressure) and Heart problem and 1 binary output variable (caesarian delivery). The outcome is to build a ML model which can  predict whether a woman needs to undergo a caesarean  delivery or not based on the input variables.**

**You can view the performance of different models in this notebook.**
 
